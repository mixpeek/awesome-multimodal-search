# Awesome Multimodal Search

*A curated list of resources for searching across multiple data modalities (images, video, audio, and documents) using AI and embedded representations.* 

## Introduction

Multimodal search enables querying and retrieving information across different data types by mapping them into a shared semantic vector space. For example, a text query can retrieve matching images, or an image can be used to find relevant text, by using deep learning models to encode each modality into high-dimensional embeddings ([Multimodal search: Searching with semantic and visual understanding - OpenSearch](https://opensearch.org/blog/multimodal-semantic-search/#:~:text=Let%E2%80%99s%20look%20closely%20at%20how,the%20score%20of%20different%20pairs)) ([Multimodal search: Searching with semantic and visual understanding - OpenSearch](https://opensearch.org/blog/multimodal-semantic-search/#:~:text=During%20model%20training%2C%20the%20image,difference%20between%20images%20and%20descriptions)). This approach goes beyond traditional keyword search, which struggles with non-text content, by representing complex data (images, audio, video) as numerical vectors that capture their meaning ([How does vector search support multimedia search?](https://blog.milvus.io/ai-quick-reference/how-does-vector-search-support-multimedia-search#:~:text=Vector%20search%20enables%20multimedia%20search,capture%20the%20actual%20content%20of)). With advances in **contrastive learning** (like OpenAI’s CLIP) and large multimodal models, it’s now possible to build unified search systems over images, videos, audio, and PDF documents. Below is an exhaustive list of libraries, services, datasets, papers, and more for multimodal search.

**Table of Contents**  
- [Libraries & Frameworks](#libraries--frameworks)  
- [Cloud Services and APIs](#cloud-services-and-apis)  
- [Research Papers](#research-papers)  
- [Datasets](#datasets)  
- [Benchmarks](#benchmarks)  
- [Tutorials and Examples](#tutorials-and-examples)  
- [Architecture & Implementation Patterns](#architecture--implementation-patterns)  

## Libraries & Frameworks

**Open-Source tools and frameworks** for building multimodal search (supporting any combination of image, video, audio, and text/document search):

- **Jina AI** – Neural search framework supporting full-text and multimodal search (text, image, audio, video) with deep learning models ([Jina - kCloudHub - Microsoft Azure Marketplace](https://azuremarketplace.microsoft.com/en-us/marketplace/apps/kcloudhub.jina?tab=overview#:~:text=Marketplace%20azuremarketplace,audio%2C%20and%20video%20data)). Provides high-level Flow API and executors for building cross-modal search applications (e.g. using CLIP, BLIP models).
- **Weaviate** – Open-source vector database with modular vectorizers for text, images, audio, and video. Its multimodal modules can ingest data in various formats and perform cross-modal searches ([Home | Weaviate](https://weaviate.io/developers/weaviate#:~:text=For%20many%2C%20data%20comes%20in,multiple%20forms%20beyond%20text)). For example, Weaviate’s `multi2vec` transformers (including integration of Meta’s ImageBind) allow combined indexing of image, audio, and text fields ([How to build an Image Search Application with Weaviate](https://weaviate.io/blog/how-to-build-an-image-search-application-with-weaviate#:~:text=vectorizer%20that%20enables%20conversion%20of,images%20to%20meaningful%20vectors)) ([Multimodal Embeddings - Weaviate](https://weaviate.io/developers/weaviate/model-providers/imagebind/embeddings-multimodal#:~:text=Multimodal%20Embeddings%20,to%20use%20the%20ImageBind)).
- **Milvus** – High-performance vector database for scalable similarity search across billions of vectors (supports embeddings from images, videos, audio, etc.). Allows building image similarity or video search systems via efficient k-NN on multimedia embeddings ([Milvus, a highly performant distributed vector database for AI apps](https://milvus.io/intro#:~:text=Milvus%2C%20a%20highly%20performant%20distributed,Search%20Relevance%20with%20Semantic)). Milvus abstracts the index layer, so any deep model (ResNet, CLIP, VGGish, etc.) can be used to generate vectors and Milvus will handle fast retrieval.
- **Elasticsearch / OpenSearch** – Popular search engines that now support dense vector fields for ANN search. By combining text indexing with vector similarity, they enable multimodal search when used with appropriate encoders ([Multimodal search: Searching with semantic and visual understanding - OpenSearch](https://opensearch.org/blog/multimodal-semantic-search/#:~:text=One%20of%20the%20most%20compelling,vectors%20that%20are%20close%20to)) ([Multimodal search: Searching with semantic and visual understanding - OpenSearch](https://opensearch.org/blog/multimodal-semantic-search/#:~:text=Let%E2%80%99s%20look%20closely%20at%20how,the%20score%20of%20different%20pairs)). (For instance, using an image embedding model to index images allows semantic image-to-image or text-to-image queries on Elasticsearch ([Multimodal search: Searching with semantic and visual understanding - OpenSearch](https://opensearch.org/blog/multimodal-semantic-search/#:~:text=,person%20or%20a%20car%20image)) ([Multimodal search: Searching with semantic and visual understanding - OpenSearch](https://opensearch.org/blog/multimodal-semantic-search/#:~:text=Three%20types%20of%20multimodal%20search)).)
- **Towhee** – An open-source machine learning pipeline framework to transform unstructured data into embeddings. Supports a wide range of modalities (images, video clips, text, audio, even 3D molecules) and offers over 140 pretrained models (e.g. CLIP for vision-language) to build multimodal pipelines ([GitHub - towhee-io/towhee: Towhee is a framework that is dedicated to making neural data processing pipelines simple and fast.](https://github.com/towhee-io/towhee#:~:text=Multi%20Modalities%3A%20Towhee%20is%20capable,Towhee%20can%20process%20them%20all)) ([GitHub - towhee-io/towhee: Towhee is a framework that is dedicated to making neural data processing pipelines simple and fast.](https://github.com/towhee-io/towhee#:~:text=Rich%20Operators%3A%20Towhee%20provides%20a,efficiently%20building%20data%20processing%20pipelines)). Towhee simplifies constructing end-to-end pipelines (decoding video frames, extracting audio features, etc., then vectorizing) for multimodal retrieval and even retrieval-augmented generation.
- **Deep Lake (Activeloop)** – A data lake for deep learning that can store raw data (images, audio, etc.) *alongside* embeddings, enabling true multimodal search on both vectors and original data ([Use ImageBind & Multimodal Retrieval for AI Image Search](https://www.activeloop.ai/resources/use-image-bind-multimodal-retrieval-for-ai-image-search/#:~:text=Unlike%20anything%20else%2C%20Deep%20Lake,tech%20products%2C%20or%20powering%20intuitive)). Deep Lake provides fast similarity search and streaming over millions of data points, with support for queries across modalities. (E.g., you can store images, their CLIP embeddings, and text in one lake and query by text or image to retrieve all relevant content.)
- **CLIP Retrieval** – Library by LAION (rom1504) for computing CLIP embeddings at scale and querying them. It powers the LAION-5B image dataset search: the system converts a text query into the CLIP vector space and performs a KNN search over billions of image embeddings ([Clip front - rom1504.github.io](https://rom1504.github.io/clip-retrieval/#:~:text=Clip%20front%20,index%20of%20clip%20image%20embedddings)). Useful for building your own image–text search backend with precomputed embeddings and faiss/ANN indexes.
- **Haystack** – NLP framework that also supports multimedia search via components like **DocumentStores** (can store PDFs and images) and **Nodes** to convert images to text (OCR) or use image embeddings. For example, using an OCR node + retriever allows image-containing documents to be searched by text. Haystack’s modular design can be extended with custom readers to handle audio (via speech-to-text) etc.
- **Other Notable Mentions**: **Vespa** (Yahoo’s engine with native tensor fields for multimodal ranking), **Annoy / FAISS** (vector indexes often used under the hood), and **Hugging Face Transformers** (provides many pretrained models – CLIP, ImageBind, Whisper, etc. – to encode different modalities for search). These building blocks are often combined to create end-to-end systems.

## Cloud Services and APIs

**Managed cloud platforms and commercial APIs** offering multimodal search capabilities or building blocks:

- **OpenAI API** – Provides state-of-the-art *multimodal* AI models and endpoints. For example, **GPT-4 Vision** can accept image inputs (e.g. for describing or analyzing an image) ([GPT-4 - OpenAI](https://openai.com/index/gpt-4-research/#:~:text=GPT,world%20scenarios)), and **Whisper** ASR transcribes audio to text ([AudioCaps: Generating Captions for Audios in The Wild](https://aclanthology.org/N19-1011.pdf#:~:text=Speech%20recognition%20and%20separation,2015%29%2C%20LS%20Speech)) (enabling audio search by converting speech to text). OpenAI’s **Embedding API** (e.g. `text-embedding-ada-002`) generates high-dimensional text embeddings commonly used for semantic document search ([CLIP embeddings to improve multimodal RAG with GPT-4 Vision | OpenAI Cookbook](https://cookbook.openai.com/examples/custom_image_embedding_search#:~:text=Multimodal%20RAG%20integrates%20additional%20modalities,textual%20data%20for%20improved%20understanding)). While OpenAI doesn’t yet offer direct image embeddings as a service, their models (CLIP, CoCa) are used in other platforms (like Google’s) and their text embeddings can be applied in cross-modal pipelines (e.g. caption images then embed the captions for search).
- **Google Vertex AI & Gemini** – Google’s cloud has introduced powerful multimodal capabilities. **Vertex AI Multimodal Embeddings** (GA release in 2023) uses the CoCa vision-language model to generate embeddings from either images or text, which can be indexed in **Vertex AI Vector Search** ([Multimodal generative AI search | Google Cloud Blog](https://cloud.google.com/blog/products/ai-machine-learning/multimodal-generative-ai-search#:~:text=years)). A demo by Google indexed 5.8 million product images with this service and enabled instantaneous text-to-image search over them ([Multimodal generative AI search | Google Cloud Blog](https://cloud.google.com/blog/products/ai-machine-learning/multimodal-generative-ai-search#:~:text=Before%20we%20go%20into%20the,code%20is%20also%20available%20here)). Coming soon is **Google Gemini**, a next-gen multimodal LLM that will accept image, video, and audio inputs. Gemini’s multimodal abilities allow understanding and integrating text, images, and even video/audio in search results ([How is Google Gemini Enhancing Search Capabilities?](https://www.thundertech.com/blog-news/how-is-google-gemini-enhancing-search-capabilities#:~:text=How%20is%20Google%20Gemini%20Enhancing,This%20enables%20a%20more)) – e.g. enabling richer Google Search responses that combine text and visual analysis.
- **Microsoft Azure** – Azure’s AI services support multimodal search through **Cognitive Services** and **Cognitive Search**. Azure’s Vision APIs (Computer Vision, Form Recognizer) can extract text from images/PDFs and detect visual features, which can be indexed for search. Recently Azure Cognitive Search added **vector search** capability, allowing indexing of image/audio/video embeddings alongside text ([Introducing Vector Search in Azure Cognitive Search | Azure Friday](https://www.youtube.com/watch?v=Bd9LWW4cxEU#:~:text=Introducing%20Vector%20Search%20in%20Azure,It)). An Azure demo shows combining OpenAI embeddings with Cognitive Search to query PDFs containing both text and images ([Azure Search Index with both text and images - Microsoft Q&A](https://learn.microsoft.com/en-us/answers/questions/1663467/azure-search-index-with-both-text-and-images#:~:text=Q%26A%20learn,only%20with%20text%20is%20straightforward)) ([Extract text from images by using AI enrichment - Azure AI Search](https://learn.microsoft.com/en-us/azure/search/cognitive-search-concept-image-scenarios#:~:text=Extract%20text%20from%20images%20by,in%20Azure%20AI%20Search%20pipelines)). Azure also offers pre-built models for OCR, face recognition, and custom vision that can be integrated into search pipelines.
- **AWS AI Services** – Amazon offers multiple APIs to enable multimodal search scenarios: **Amazon Rekognition** for image and video analysis (object detection, face search, text in images) can be used to index visual content ([Amazon Rekognition – frequently asked questions - AWS](https://aws.amazon.com/rekognition/faqs/#:~:text=Amazon%20Rekognition%20is%20a%20service,and%20helps%20you%20analyze%20them)) ([Amazon Rekognition – frequently asked questions - AWS](https://aws.amazon.com/rekognition/faqs/#:~:text=video%20even%20when%20their%20faces,that%20make%20video%20search%20easy)). Rekognition Image, for instance, can detect labels or extract text and make a large image collection searchable by these attributes ([Amazon Rekognition – frequently asked questions - AWS](https://aws.amazon.com/rekognition/faqs/#:~:text=Amazon%20Rekognition%20is%20a%20service,and%20helps%20you%20analyze%20them)). **Amazon Kendra** is an AI-powered enterprise search that indexes documents (PDF, Word, HTML, etc.) and provides semantic search over them ([Generative AI and multi-modal agents in AWS: The key to unlocking ...](https://aws.amazon.com/blogs/machine-learning/generative-ai-and-multi-modal-agents-in-aws-the-key-to-unlocking-new-value-in-financial-markets/#:~:text=,sources%2C%20including%20documents%20and)). Kendra can be extended with OCR for scanned images in PDFs ([Enable Amazon Kendra search for a scanned or image-based text ...](https://aws.amazon.com/blogs/machine-learning/enable-amazon-kendra-search-for-a-scanned-or-image-based-text-document/#:~:text=Enable%20Amazon%20Kendra%20search%20for,JPEG%2C%20PNG%2C%20or%20PDF)), and it uses ML to rank results by intent rather than keywords ([[PDF] Amazon Kendra - Developer Guide](https://docs.aws.amazon.com/pdfs/kendra/latest/dg/kendra-dg.pdf#:~:text=%5BPDF%5D%20Amazon%20Kendra%20,keyword%20searches%2C%20Amazon%20Kendra)). AWS also has **Transcribe** (speech-to-text) to turn audio into searchable text, and **Lex** for voice queries – which together can enable voice-to-document search pipelines.
- **Stability AI ClipDrop** – The ClipDrop API by Stability AI offers a suite of visual intelligence tools (image upscaling, background removal, object replacement, etc.) ([Create stunning visuals in seconds with AI.](https://clipdrop.co/#:~:text=Generative%20Fill%20Generative%20Fill%20Replace%2C,by%202x%20or%204x%20in)). While not a search engine on its own, it provides **image processing APIs** that can assist multimodal search workflows – for example, cleaning up user-supplied images or generating variations. ClipDrop’s API can be integrated to preprocess images (e.g. remove backgrounds) before using them as search queries. It demonstrates how generative and enhancement tools can complement search (by normalizing inputs or augmenting the dataset). *ClipDrop is an example of how cloud vision AI can be used alongside search pipelines* ([Create stunning visuals in seconds with AI.](https://clipdrop.co/#:~:text=ADD%20MAGIC%20TO%20YOUR%20OWN,APPS%20WITH%20THE%20CLIPDROP%20API)).
- **Others**: **IBM Watson Discovery** (indexes enterprise docs with NLP and can handle some images), **Cohere** (provides multilingual text embeddings and classification that could be used for document search), and **Hugging Face Hub/Inference API** (hosts many multimodal models – one can call these via API or use hosted spaces to perform tasks like CLIP image search or audio-text retrieval). **Bing Visual Search API** and **Google Lens API** also allow image-based queries (finding similar images, identifying objects) which can be considered specialized “search” endpoints for the image modality.

## Research Papers

**Key research papers** advancing multimodal retrieval and representation learning (grouped by modality focus):

- **CLIP (ICML 2021)** – *“Learning Transferable Visual Models From Natural Language Supervision”*, Radford et al. ([Multimodal search: Searching with semantic and visual understanding - OpenSearch](https://opensearch.org/blog/multimodal-semantic-search/#:~:text=Let%E2%80%99s%20look%20closely%20at%20how,the%20score%20of%20different%20pairs)) ([Multimodal search: Searching with semantic and visual understanding - OpenSearch](https://opensearch.org/blog/multimodal-semantic-search/#:~:text=During%20model%20training%2C%20the%20image,difference%20between%20images%20and%20descriptions)). OpenAI’s landmark two-tower model that jointly trains an image encoder and text encoder on 400M image-text pairs. CLIP embeds images and captions into a shared space, aligning them such that related images and text are close by cosine similarity. It demonstrated astounding zero-shot image retrieval and classification capabilities, and has become a foundation for multimodal search (enabling text-to-image and image-to-text retrieval).
- **ALIGN (ICML 2021)** – *“Scaling Up Visual and Vision-Language Representation Learning”*, Jia et al. Google’s model similar to CLIP (trained on 1.8B image-text pairs) that achieved state-of-the-art on image-text retrieval benchmarks. Introduced at the same time as CLIP, it showed the benefit of massive noisy data for representation learning.
- **BLIP-2 (2023)** – *“Bootstrapping Language-Image Pre-training”*, Li et al. Salesforce’s vision-language model that connects a pretrained vision encoder with a frozen LLM via a Q-Former. Notably, BLIP-2 ViT-G achieved state-of-the-art results on MS COCO image-text retrieval (Recall@1 ~ **91%** on COCO Captions) after fine-tuning ([MS COCO Benchmark (Image-to-Text Retrieval) - Papers With Code](https://paperswithcode.com/sota/image-to-text-retrieval-on-coco?metric=Recall%405#:~:text=Code%20paperswithcode,of%209%20papers%20with%20code)). It is an example of using a generative model for strong retrieval performance, and supports image-to-text and text-to-image search.
- **ImageBind (ICML 2023)** – *“ImageBind: One Embedding Space To Bind Them All”*, Girdhar et al. ([Use ImageBind & Multimodal Retrieval for AI Image Search](https://www.activeloop.ai/resources/use-image-bind-multimodal-retrieval-for-ai-image-search/#:~:text=To%20achieve%20this%2C%20we%20will,a%20vanilla%20image%20similarity%20search)) ([Use ImageBind & Multimodal Retrieval for AI Image Search](https://www.activeloop.ai/resources/use-image-bind-multimodal-retrieval-for-ai-image-search/#:~:text=Unlike%20anything%20else%2C%20Deep%20Lake,tech%20products%2C%20or%20powering%20intuitive)). Meta AI’s model that extends the CLIP concept to **six modalities**: image, text, audio, video (motion), depth, and thermal. ImageBind learns a single latent space that binds modalities without needing paired data for every modality (only images and their aligned modalities). It enables cross-modal retrieval like using an audio clip as a query to find relevant images, etc. ImageBind is a game-changer for holistic multimodal search since one model can embed vastly different inputs into one common space.
- **CLAP (NeurIPS 2022)** – *“Large-Scale Contrastive Language-Audio Pretraining”*, Wu et al. ([](https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Wu_100_t6b.pdf#:~:text=mixed%20dataset%20consisting%20of%20over,Terms%E2%80%94%20Contrastive%20Learning%2C%20Audio%20Understanding)) ([](https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Wu_100_t6b.pdf#:~:text=In%20this%20report%2C%20we%20follow,learning%20or%20expanding%20CLIP%20to)). A CLIP-like approach for audio and text. Trained on the LAION-Audio dataset (630k audio clips with captions) and other audio data, CLAP produces embeddings that enable text-to-audio and audio-to-text retrieval. It achieved top results on AudioCaps and Clotho retrieval tasks, significantly outperforming previous audio-only models in recall and mAP. CLAP demonstrated the viability of bridging audio and language with contrastive learning, analogous to image-text models.
- **CLIP4Clip (EMNLP 2021)** – *“An Empirical Study of CLIP for End-to-End Video Retrieval”*, Luo et al. ([An Empirical Study of CLIP for End to End Video Clip Retrieval - arXiv](https://arxiv.org/abs/2104.08860#:~:text=arXiv%20arxiv,end%20manner)). This work adapted CLIP for video **clip** retrieval by extending it to encode video frames (with temporal modeling) and aligning them with text. It showed that transferring image-text knowledge (from CLIP) to video-text tasks yields strong performance on MSR-VTT and ActivityNet. CLIP4Clip was among the first to achieve high retrieval accuracy on video-text benchmarks by building on a pretrained multimodal model.
- **Frozen in Time (CVPR 2021)** – Bain et al. Proposed a dual encoder for video and text trained on image-text data plus unsupervised video, “freezing” image encoders to apply to video. Demonstrated the benefit of combining image and video resources for video retrieval. This influenced later works like VideoCLIP.
- **X-CLIP (ECCV 2022)** – Ni et al. Improved video-text retrieval by using a CLIP-based video encoder with cross-frame attention, capturing temporal cues. Achieved state-of-art on MSR-VTT (at the time) with better modeling of motion for matching text queries.
- **LayoutLMv3 (ACL 2022)** – Huang et al. from Microsoft. A **document multimodal** model that incorporates text, layout (2D position), and image of scanned documents in a unified Transformer ([purnasankar300/layoutlmv3: Large-scale Self-supervised Pre ...](https://github.com/purnasankar300/layoutlmv3#:~:text=purnasankar300%2Flayoutlmv3%3A%20Large,LayoutXLM%3A%20multimodal)). While aimed at document understanding (form parsing, PDF question-answering), it provides embeddings that combine textual content with the visual layout. This is valuable for document search, since standard text embeddings lose layout information. Models like LayoutLM can enable searching scanned documents or forms by both textual and structural content (e.g. find a form where “Name” field is John Doe).
- **Document Screenshot Embedding (EMNLP 2024)** – *“Unifying Multimodal Retrieval via Document Screenshot Embedding (DSE)”*, Ma et al. ([](https://aclanthology.org/2024.emnlp-main.373.pdf#:~:text=Screenshot%20Embedding%20,page%20screenshots%20as%20the%20corpus)) ([](https://aclanthology.org/2024.emnlp-main.373.pdf#:~:text=to%20answer%20the%20questions%20from,These%20experiments)). An innovative approach to document retrieval: instead of parsing PDF/Twitter/web pages into text and images separately, it treats a whole page screenshot as one input to a vision-language model. DSE encodes the entire page image (which contains text, figures, layout) into an embedding, allowing retrieval of documents by text queries without explicit OCR. On a 1.3M screenshot corpus (Wiki-SS), DSE outperformed traditional text-only retrieval by a large margin ([](https://aclanthology.org/2024.emnlp-main.373.pdf#:~:text=to%20answer%20the%20questions%20from,These%20experiments)), showing a new paradigm where *“the document is the image.”* This research hints at future systems where complex documents are indexed via multimodal encoders that preserve all information.
- **PaLI (CVPR 2023)** – *“PaLI: A Jointly-Scaled Multilingual Language-Image Model”*, Chen et al. Google’s 55B parameter model handling image and text across 100+ languages. While primarily for captioning/QA, PaLI achieved strong retrieval results, indicating scaling helps multi-language cross-modal search.
- *([See also:](https://paperswithcode.com/task/cross-modal-retrieval) other impactful papers like VSE++, VLMo, Florence, ALPRO, VATT, VLP, etc. covering various aspects of multimodal learning. The above selection emphasizes recent advances and variety.)*

## Datasets

**Datasets for training and evaluating** multimodal search models, across vision, audio, and document domains:

- **MS COCO Captions** – Microsoft COCO is a widely-used dataset of **~330K images** each with 5 textual captions (in English) ([AudioCaps: Generating Captions for Audios in The Wild](https://aclanthology.org/N19-1011.pdf#:~:text=visual%20domain,2017%29%20and)). Originally for image captioning, COCO is the standard benchmark for image-text retrieval (with 5K images in the test set used for Recall@K evaluations). Many models (CLIP, etc.) report results on COCO; e.g., top models exceed 90% R@10 for both text→image and image→text retrieval.
- **Flickr30k** – 31K images from Flickr, each with 5 crowd-sourced captions ([AudioCaps: Generating Captions for Audios in The Wild](https://aclanthology.org/N19-1011.pdf#:~:text=visual%20domain,2017%29%20and)). Often used alongside COCO for evaluating cross-modal retrieval. The Flickr30k Entities extension also provides region-level annotations (useful for localized search).
- **LAION-5B** – The largest open multi-modal dataset: **5.85 billion** image–alt-text pairs scraped from the web and filtered by CLIP scores ([LAION-5B: An open large-scale dataset for training next generation ...](https://arxiv.org/abs/2210.08402#:~:text=We%20present%20LAION,32B%20contain%20English%20language)). Subsets include LAION-2B (English-only, 2.3B pairs) and LAION-400M. This dataset enabled training of CLIP at unprecedented scale and powers many *open* models (Stable Diffusion’s image encoder was trained on LAION). It’s not an evaluation set per se, but a treasure trove for training and for building *web-scale* image-text search engines.
- **MSR-VTT** – A video–text dataset of **10K video clips** (mostly 10–30s each) from YouTube, with **200K captions** (≈20 per clip) covering diverse categories ([The MSR-Video to Text dataset with clean annotations - ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S107731422200159X#:~:text=The%20MSR,Each%20video)). It’s the de facto benchmark for text-to-video retrieval; most recent papers report R@1 on the 1K test split (for instance, CLIP4Clip ~46% R@1, and newer models ~50–60%). It is also used for video captioning tasks.
- **ActivityNet Captions** – 20K YouTube videos (total 849 hours) with temporally localized descriptions (average 3.65 captions per video). Often used for dense video retrieval or story-based retrieval, where a query might match a particular moment in a video.
- **Flickr Audio Caption Corpus** – Extension of Flickr8k where spoken descriptions are recorded by humans (which introduces natural speech variations). Useful for *spoken* query -> image search or vice versa. (Similarly, MS COCO’s captions have been read aloud in some works to create spoken query datasets.)
- **AudioCaps** – **46K** short audio clips from YouTube (10s each) with crowdsourced captions ([AudioCaps: Generating Captions for Audios in The Wild](https://aclanthology.org/N19-1011.pdf#:~:text=We%20contribute%20its%20first%20large,represen%02tations%20and%20captioning%20models%20are)). Created by extending the AudioSet ontology with natural language descriptions. This is a primary benchmark for text↔audio retrieval and audio captioning. State-of-art models (like CLAP) on AudioCaps have significantly improved text-to-audio retrieval metrics in recent years.
- **Clotho** – An audio captioning and retrieval dataset with **3.8K audio clips** (~15–30s) each paired with 5 captions ([](https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Wu_100_t6b.pdf#:~:text=Clotho%20,5%2C733%20158.15%20Filename%2C%20audio)). It was used in the DCASE challenge for *“language-based audio retrieval.”* Clotho is smaller, but difficult, making it a popular evaluation set for audio-text models (often testing zero-shot retrieval or few-shot learning).
- **LAION-Audio** – A large-scale audio-text dataset (by LAION) consisting of **>630K** audio-text pairs (captions for AudioSet clips, etc.). Used to train the CLAP model ([](https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Wu_100_t6b.pdf#:~:text=In%20this%20report%2C%20we%20follow,learning%20or%20expanding%20CLIP%20to)). It’s not an evaluation benchmark itself, but important for anyone training new audio-text embedding models for search.
- **Wikipedia (Wiki-SS)** – The **Wiki Screenshots (Wiki-SS)** dataset introduced by Ma et al. (2024) contains **1.3 million** screenshots of Wikipedia pages along with the text of the page ([](https://aclanthology.org/2024.emnlp-main.373.pdf#:~:text=a%20large%20vision,1%20retrieval)). Each screenshot is a full rendering of a page, used in the DSE paper to enable image-based document retrieval. This dataset is valuable for researching document search that accounts for layout, images, and text together.
- **BEIR Benchmark** – A **benchmark suite of 18 information retrieval datasets** spanning diverse domains and tasks ([[PDF] BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of ...](https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/65b9eea6e1cc6bb9f0cd2a47751a186f-Paper-round2.pdf#:~:text=,datasets%20for%20comparison%20and)) (including web search, QA, tweets, news, biomedical, etc.). While BEIR is text-focused, it’s essential for evaluating the *text* embedding quality of multimodal models when retrieving documents. Many multimodal models (e.g., CLIP) are now also being tested on subsets of BEIR by converting image or audio queries to text, etc., to assess zero-shot adaptability.
- **Document QA Datasets** – These include **Natural Questions** (real Google queries with Wikipedia answers), **TriviaQA**, **SQuAD**, and others which, although QA-oriented, double as retrieval corpora (Wikipedia or web). Often, a document RAG system will be evaluated on how well it retrieves relevant passages for these questions. They serve as indirect benchmarks for document search quality in a multimodal assistant (especially when queries might be images or speech).
- **Multimodal Conversational**: Datasets like **OK-VQA**, **FVQA** (visual question answering requiring external knowledge) and **AudioQA** are emerging, which require retrieving from text knowledge bases using image/audio queries. These push the boundaries of multimodal retrieval + reasoning.
- *For a comprehensive list of datasets, see surveys like “Multimodal Learning: A Survey on Datasets” (2023) ([Survey of Large Multimodal Model Datasets, Application Categories ...](https://arxiv.org/html/2412.17759v1#:~:text=Survey%20of%20Large%20Multimodal%20Model,each%20unique%20to%20one)), which catalogs dozens of image, video, and audio captioning/retrieval datasets beyond those above.*

## Benchmarks

**Evaluation metrics and leaderboards** for multimodal search are crucial to measure progress:

- **Common Metrics** – The standard metrics for retrieval are used across modalities: **Recall@K** (R@1, R@5, R@10) – the percentage of queries for which the correct item is among the top *K* results, **Median/Mean Rank** of the true match, and **Mean Average Precision** (mAP). For instance, in text-to-audio retrieval, DCASE Challenge uses R@10 and mAP@10 (average precision of top 10 results) ([](https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Wu_100_t6b.pdf#:~:text=R%4010%2C%20and%20mAP%4010%20for%20both,the%20ground%20truth%20for%20a)). A strong model on the Clotho audio retrieval task achieves ~45% R@10 and mAP@10 ≈0.21 ([](https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Wu_100_t6b.pdf#:~:text=Model%20Mean%20Rank%20Median%20Rank,211)), compared to a baseline ~7% mAP@10 ([](https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Wu_100_t6b.pdf#:~:text=match%20at%20L374%20Model%20Mean,211)) – a massive improvement showing the impact of learned audio-text embeddings.
- **Image-Text Retrieval** – On MS COCO, results are reported for 5K test images (with 5 captions each) in two tasks: text→image and image→text. Recall@1 is very competitive; as of 2023, fine-tuned models like BLIP-2 ViT-G have exceeded **80-85% R@1** on COCO text→image ([MS COCO Benchmark (Image-to-Text Retrieval) - Papers With Code](https://paperswithcode.com/sota/image-to-text-retrieval-on-coco?metric=Recall%405#:~:text=Code%20paperswithcode,of%209%20papers%20with%20code)) (and ~97% R@10). Flickr30k is slightly easier (often >90% R@1). Leaderboards are tracked on **Papers With Code** – e.g. the current SOTA on COCO is BLIP-2 (ViT-G) ([MS COCO Benchmark (Image-to-Text Retrieval) - Papers With Code](https://paperswithcode.com/sota/image-to-text-retrieval-on-coco?metric=Recall%405#:~:text=Code%20paperswithcode,of%209%20papers%20with%20code)). These high numbers indicate the model retrieves the correct image for a caption most of the time. For zero-shot (without dataset-specific fine-tuning), CLIP and ALIGN originally achieved ~58-68% R@1 on COCO, which was a revolution at the time.
- **Video-Text Retrieval** – Benchmarks like MSR-VTT (1K test set) report Recall@1,5,10 of retrieving the correct video for a text query. Recent models have pushed R@1 above 50%. For example, **GRAM** (2023) is a top performer on MSR-VTT ([MSR-VTT Benchmark (Video Retrieval) - Papers With Code](https://paperswithcode.com/sota/video-retrieval-on-msr-vtt#:~:text=MSR,of%2040%20papers%20with%20code)), and Twelve Labs’ Marengo-2.6 (2024) reports surpassing prior SOTA by +10% R@1 on MSR-VTT and +3% on ActivityNet in zero-shot mode ([Introducing Marengo-2.6: A New State-of-the-Art Video Foundation Model for Any-to-Any Search - Twelve Labs](https://www.twelvelabs.io/blog/introducing-marengo-2-6#:~:text=%2A%20New%20State,reasoning%20abilities%20across%20multiple%20modalities)). In fact, Marengo achieves a new SOTA with ~60% R@1 on MSR-VTT, and also set records on ActivityNet Captions (a larger test) ([Introducing Marengo-2.6: A New State-of-the-Art Video Foundation Model for Any-to-Any Search - Twelve Labs](https://www.twelvelabs.io/blog/introducing-marengo-2-6#:~:text=%2A%20New%20State,can%20exhibit%20impressive%20perceptual%20reasoning)). The progress is tracked on public leaderboards (e.g., MSR-VTT on Papers With Code, MSVD, LSMDC). The **average recall** (mean of R@1 and R@5) is sometimes used to compare models – Marengo-2.6 improved MSR-VTT average recall by +4% ([Introducing Marengo-2.6: A New State-of-the-Art Video Foundation Model for Any-to-Any Search - Twelve Labs](https://www.twelvelabs.io/blog/introducing-marengo-2-6#:~:text=Image)) over previous models.
- **Audio-Text Retrieval** – Benchmarks like Clotho and AudioCaps use R@N and mAP. Due to the difficulty of audio semantics, numbers are generally lower. Top models achieve ~20-25% R@1 on AudioCaps and higher on AudioCaps if captions are longer. DCASE Challenge leaderboards show steady improvement – in 2022, the best system had mAP@10=0.214 on Clotho ([](https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Wu_100_t6b.pdf#:~:text=dataset%20,Dataset)) ([](https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Wu_100_t6b.pdf#:~:text=Model%20Mean%20Rank%20Median%20Rank,211)). For a qualitative sense: an R@1 of 25% means 1 in 4 audio queries find the correct clip at rank-1, which is remarkable given the richness of sound.
- **Document Retrieval** – Often measured by metrics like **NDCG@K** (Normalized Discounted Cumulative Gain) and MRR (Mean Reciprocal Rank) especially in enterprise and academic search. The BEIR benchmark uses NDCG@10 to evaluate how well models retrieve relevant documents across 18 datasets ([[PDF] BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of ...](https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/65b9eea6e1cc6bb9f0cd2a47751a186f-Paper-round2.pdf#:~:text=,datasets%20for%20comparison%20and)). In the context of multimodal search, if the query is text this reduces to a text IR task (where models like Contriever, GTR, etc., excel). But if queries are images or mixed (e.g. a chart image as a query to find relevant docs), new metrics are explored. The Wiki-SS dataset introduced **Top-1 accuracy** on Natural Questions when using screenshots: DSE achieved 49% top-1, vs 32% for BM25 text search ([](https://aclanthology.org/2024.emnlp-main.373.pdf#:~:text=to%20answer%20the%20questions%20from,These%20experiments)). Enterprise benchmarks (like **MS MARCO** for passage ranking) also influence evaluation of embeddings used in RAG systems.
- **Leaderboards** – Public leaderboards for cross-modal retrieval tasks can be found on sites like **Papers With Code** and **EvalAI**. For example, **MSR-VTT Video Retrieval** has dozens of entries (with GRAM and others at the top in 2023) ([MSR-VTT Benchmark (Video Retrieval) - Papers With Code](https://paperswithcode.com/sota/video-retrieval-on-msr-vtt#:~:text=MSR,of%2040%20papers%20with%20code)). **COCO Retrieval** leaderboards are often reported in papers or on competition websites (the latest COCO Caption Challenge). DCASE hosts a leaderboard for audio retrieval each year. These provide a way to compare models’ performance under identical conditions.
- **Metrics for Multimodal Systems** – Beyond retrieval accuracy, integrated systems (like search combined with QA) introduce metrics such as answer accuracy or BLEU scores when generating outputs. For instance, a multimodal chatbot might be evaluated on whether it retrieved the right image and answered correctly. However, at the core, the retrieval metrics above remain fundamental for the search component.

In summary, retrieval performance has dramatically improved with multimodal embeddings. Text-image retrieval is nearing saturation on some benchmarks, while video and audio retrieval still have headroom. Ongoing competitions and benchmarks (e.g. **TACT for text-audio retrieval**, **Video Browser Showdown**, etc.) continue to push the state-of-the-art. For up-to-date SOTA values, see the Papers with Code pages for [Image-Text Retrieval](https://paperswithcode.com/sota/image-captioning-on-coco-images) and [Video Retrieval](https://paperswithcode.com/sota/video-retrieval-on-msr-vtt).

## Tutorials and Examples

**Educational resources, code repos, and demos** to learn and implement multimodal search:

- **Building a Multimodal Search Engine with ImageBind & Deep Lake** – Activeloop’s in-depth tutorial ([Use ImageBind & Multimodal Retrieval for AI Image Search](https://www.activeloop.ai/resources/use-image-bind-multimodal-retrieval-for-ai-image-search/#:~:text=In%20this%20guide%2C%20we%E2%80%99ll%20explore,user%20experience%2C%20and%20business%20intelligence)) ([Use ImageBind & Multimodal Retrieval for AI Image Search](https://www.activeloop.ai/resources/use-image-bind-multimodal-retrieval-for-ai-image-search/#:~:text=Unlike%20anything%20else%2C%20Deep%20Lake,tech%20products%2C%20or%20powering%20intuitive)) shows how to create a cross-modal search engine that can take a text, an image, or even an audio clip as a query and retrieve relevant *AI-generated images*. It demonstrates using Meta’s ImageBind to embed all modalities into one space, storing them in Deep Lake, and querying. This guide is great for learning how to extend image search to other modalities and how to handle data pipelines (e.g., segmenting video into frames, audio into segments).
- **Multimodal Search with OpenAI CLIP (Kaggle Notebook)** – A Kaggle example by @heyytanay ([Multimodal Search using OpenAI CLIP and LanceDB - Kaggle](https://www.kaggle.com/code/heyytanay/multimodal-search-using-openai-clip-and-lancedb#:~:text=In%20this%20notebook%2C%20we%20will,dataset%2C%20store%20it%20into)) that walks through using CLIP to embed a set of images, store embeddings in LanceDB (a vector database), and perform text-to-image search. It’s a concise introduction to the core idea of embedding-based search, and shows how to use a smaller dataset and basic ANN retrieval for quick prototyping.
- **Jina AI Hello Multimodal** – Jina’s example project (available via `jina hello multimodal` and explained in a Medium article) demonstrates indexing a collection of *multimodal documents* (with text + images) and querying with either modality ([Multimodal Search Demo in Detail. Let’s see how we can search images and… | by Susana G | Medium](https://medium.com/jina-ai/multimodal-search-demo-in-detail-b2c21a5a21a1#:~:text=Multimodality%20basically%20means%20multiple%20modalities,with%20multimodality%3A%20text%20and%20images)) ([Multimodal Search Demo in Detail. Let’s see how we can search images and… | by Susana G | Medium](https://medium.com/jina-ai/multimodal-search-demo-in-detail-b2c21a5a21a1#:~:text=Let%E2%80%99s%20say%20you%20have%20the,when%20multimodal%20search%20is%20useful)). For instance, you can provide an image and a text snippet together as a query to refine search results. This showcases Jina’s flow architecture, with parallel encoders for each modality and a merger. It’s a helpful resource to learn about merging multi-modal queries and results.
- **Pinecone & CLIP Blog/Colab** – *“Multi-modal Search with CLIP”* ([Multi-modal ML with OpenAI's CLIP | Pinecone](https://www.pinecone.io/learn/series/image-search/clip/#:~:text=The%20multi,representation%20of%20the%20respective%20input)) ([Multi-modal ML with OpenAI's CLIP | Pinecone](https://www.pinecone.io/learn/series/image-search/clip/#:~:text=Image%3A%20Similar%20text%20and%20images,share%20a%20similar%20vector%20space)) on Pinecone’s blog explains CLIP’s dual encoder architecture and provides code to use CLIP embeddings with a Pinecone index. It includes an interactive demo where you can type a phrase and retrieve matching images (content-based image retrieval without keywords). This is useful to understand production deployment of neural search with a managed vector DB.
- **OpenAI Cookbook – Multimodal RAG** – OpenAI’s cookbook has an example *“CLIP embeddings to improve multimodal RAG with GPT-4 Vision”* ([CLIP embeddings to improve multimodal RAG with GPT-4 Vision | OpenAI Cookbook](https://cookbook.openai.com/examples/custom_image_embedding_search#:~:text=Multimodal%20RAG%20integrates%20additional%20modalities,textual%20data%20for%20improved%20understanding)). It outlines how to build a Retrieval-Augmented Generation system that can accept image queries. The notebook demonstrates extracting image features with CLIP, performing similarity search, and then feeding both the query image and retrieved text info into GPT-4 for answer generation ([CLIP embeddings to improve multimodal RAG with GPT-4 Vision | OpenAI Cookbook](https://cookbook.openai.com/examples/custom_image_embedding_search#:~:text=Adopting%20the%20approach%20from%20the,captioning%2C%20to%20boost%20retrieval%20accuracy)). This tutorial marries search with generation, highlighting an implementation pattern where images are used to retrieve relevant text (or other images) as context.
- **Weaviate Multimodal RAG Demo** – Weaviate’s documentation and blog provide a step-by-step guide to perform multimodal Retrieval-Augmented Generation ([An Easy Introduction to Multimodal Retrieval-Augmented Generation ...](https://developer.nvidia.com/blog/an-easy-introduction-to-multimodal-retrieval-augmented-generation-for-video-and-audio/#:~:text=An%20Easy%20Introduction%20to%20Multimodal,to%20search%20information%20in%20videos)), including how to index data with images and text, then ask questions that require retrieving both. One example is storing product images with descriptions and asking an LLM “Find items that look like [image] and have [attribute]”.
- **YouTube: “Search All – Cross-Modal Retrieval with ImageBind and Deep Lake”** – A recorded project walkthrough (by Activeloop) showing a working system where you can input an audio (e.g. humming a tune) and find matching images and videos. It’s a practical demonstration of the promise of unified embeddings. Great for inspiration and seeing a multimodal search UI in action.
- **Cross-Modal E-commerce Search** – Anyscale’s blog post *“Building and Scaling a Cross-Modal Search for E-commerce”* ([Cross-modal Search for E-commerce: Building and Scaling a Cross ...](https://www.anyscale.com/blog/cross-modal-search-for-e-commerce-building-and-scaling-a-cross-modal-image-retrieval-app#:~:text=Cross,more%20intuitive%20and%20accurate)) describes how CLIP was used to let users search clothing images with text descriptions (“red summer dress”) and even refine searches by image similarity. It discusses system design (embedding service, index service, etc.) and how to handle the large scale (millions of product images). This is a concrete real-world use case blueprint.
- **Colab: Multilingual Image Search** – A Colab by LAION showing how to use CLIP and the LAION-5B index to perform multilingual search (query in one language, find images with captions in another language). It illustrates the multilingual aspect of multimodal search, using CLIP’s ability to work with non-English text.
- **Academic Talks and Workshops** – Check out *CVPR tutorials* on multimodal learning, the *DCASE workshop* for audio retrieval insights, and the *Video Retrieval Tutorial (ICMR)* for deep dives into video search. These often share code and slides with valuable tips.
- **Full Stack Demos** – Repos like **Photoroom’s Multimodal Search** (which combines CLIP and OCR to let users search their photo collections by describing image content), or **Xiaoice’s Qiwen system** (a Chinese multimodal dialog that searches images/GIFs to respond) are enlightening to study. They showcase how to integrate multiple components – embedding models, OCR for text in images, ASR for voice queries, etc. – into one product.
- **Community Projects** – Many open-source examples exist: e.g., a *“CLIP-based Video Search”* where keyframes are indexed with CLIP; *audio sample search* using spectrogram embeddings; *PDF semantic search* combining layout analysis with embeddings. Browsing GitHub (search terms like “multimodal search”, “CLIP retrieval”) yields lots of real projects to learn from.

By exploring these resources, one can learn best practices such as indexing strategy (per-modality indexes or unified index), handling of modality-specific preprocessing (like sampling video frames or using Whisper to index audio), and user interface ideas (e.g., presenting a combined gallery of images and text results). The field is very hands-on, and these examples help ground the theory in practical implementations.

## Architecture & Implementation Patterns

When building a multimodal search system, certain architecture patterns have emerged as effective. Two notable ones are **two-tower encoders for retrieval** and **retrieval-augmented generation (RAG)** for using search in QA/chat systems. Below we highlight these with an example diagram:

 ([An Easy Introduction to Multimodal Retrieval-Augmented Generation | NVIDIA Technical Blog](https://developer.nvidia.com/blog/an-easy-introduction-to-multimodal-retrieval-augmented-generation/)) *Multimodal retrieval-augmented generation pipeline (example from NVIDIA): a user query is processed by an LLM and embedding model to retrieve relevant text *and* image chunks, which are then fused to produce a final answer ([An Easy Introduction to Multimodal Retrieval-Augmented Generation | NVIDIA Technical Blog](https://developer.nvidia.com/blog/an-easy-introduction-to-multimodal-retrieval-augmented-generation/#:~:text=When%20a%20user%20prompts%20the,for%20generating%20the%20final%20response)) ([An Easy Introduction to Multimodal Retrieval-Augmented Generation | NVIDIA Technical Blog](https://developer.nvidia.com/blog/an-easy-introduction-to-multimodal-retrieval-augmented-generation/#:~:text=Image%3A%20Diagram%20shows%20a%20workflow,retrieving%20Information%20from%20multimodal%20data)).*

- **Dual Encoder Retrieval** – For pure search applications, the standard is a two-tower model (as illustrated by CLIP) where each modality has its own encoder, and similarity is computed in a shared vector space ([Multimodal search: Searching with semantic and visual understanding - OpenSearch](https://opensearch.org/blog/multimodal-semantic-search/#:~:text=Let%E2%80%99s%20look%20closely%20at%20how,the%20score%20of%20different%20pairs)). This architecture is scalable: all database items (images, audio, etc.) are encoded offline and stored as vectors; queries are encoded on the fly and a vector search finds nearest neighbors. The key design questions are how to train such encoders (contrastive learning on paired data is common), and how many modalities to include. Recent research (like ImageBind) suggests a single unified encoder for all modalities is possible, but many systems still use separate encoders per modality and then either concatenate the vectors or search within each modality’s index.
- **Modality Fusion and Re-Ranking** – Sometimes a query or item contains multiple modalities (e.g. a product listing has an image and description). Architecture patterns here include **early fusion** (combine modalities into one embedding, e.g. by averaging image and text embeddings) or **late fusion** (do separate searches by each modality then intersect or re-rank results). The OpenSearch example of (image+text)→image search ([Multimodal search: Searching with semantic and visual understanding - OpenSearch](https://opensearch.org/blog/multimodal-semantic-search/#:~:text=provided%20image)) ([Multimodal search: Searching with semantic and visual understanding - OpenSearch](https://opensearch.org/blog/multimodal-semantic-search/#:~:text=Combining%20the%20two%20search%20types%3A,image%20search)) shows how an image query can be “augmented” with text to narrow results. In practice, late fusion might take the intersection of text and image search results or weight their scores.
- **Indexing Strategies** – A practical architectural aspect is how to index multi-modal data. One approach is a **joint index**: e.g., store all items as a combination of their text vector (for text queries) and image vector (for image queries) in the same index. Another is **separate indexes** per modality: e.g., one ANN index for image embeddings, one for text embeddings, etc., and route queries accordingly. If users may submit multi-modal queries (say an image with a textual filter), you might query both indexes and take an intersection. Systems like Jina allow parallel flows for this ([Multimodal Search Demo in Detail. Let’s see how we can search images and… | by Susana G | Medium](https://medium.com/jina-ai/multimodal-search-demo-in-detail-b2c21a5a21a1#:~:text=Flow%3A)).
- **RAG (Retrieval-Augmented Generation)** – This pattern integrates search with a generative model to handle complex queries, especially in QA or assistant applications. The diagram above shows a typical multimodal RAG flow ([An Easy Introduction to Multimodal Retrieval-Augmented Generation | NVIDIA Technical Blog](https://developer.nvidia.com/blog/an-easy-introduction-to-multimodal-retrieval-augmented-generation/#:~:text=When%20a%20user%20prompts%20the,for%20generating%20the%20final%20response)) ([An Easy Introduction to Multimodal Retrieval-Augmented Generation | NVIDIA Technical Blog](https://developer.nvidia.com/blog/an-easy-introduction-to-multimodal-retrieval-augmented-generation/#:~:text=Image%3A%20Diagram%20shows%20a%20workflow,retrieving%20Information%20from%20multimodal%20data)): a user query (which could include text and an image) is first broken down – the image might be processed by a Vision-Question Answering model to extract a description or relevant info ([An Easy Introduction to Multimodal Retrieval-Augmented Generation | NVIDIA Technical Blog](https://developer.nvidia.com/blog/an-easy-introduction-to-multimodal-retrieval-augmented-generation/#:~:text=Image%3A%20Diagram%20shows%20a%20workflow,retrieving%20Information%20from%20multimodal%20data)). Meanwhile, the text query is embedded (and possibly concatenated with the image info) and used to retrieve relevant documents from a vector store (which may contain text chunks, images, or both) ([An Easy Introduction to Multimodal Retrieval-Augmented Generation | NVIDIA Technical Blog](https://developer.nvidia.com/blog/an-easy-introduction-to-multimodal-retrieval-augmented-generation/#:~:text=When%20a%20user%20prompts%20the,for%20generating%20the%20final%20response)). Retrieved results are then fed into an LLM along with the query to generate a final answer or result. This pattern is seen in systems like Bing Chat (which can take images as part of the prompt) and research prototypes in papers ([An Easy Introduction to Multimodal Retrieval-Augmented Generation | NVIDIA Technical Blog](https://developer.nvidia.com/blog/an-easy-introduction-to-multimodal-retrieval-augmented-generation/#:~:text=A%20retrieval,and%20other%20forms%20of%20information)) ([An Easy Introduction to Multimodal Retrieval-Augmented Generation | NVIDIA Technical Blog](https://developer.nvidia.com/blog/an-easy-introduction-to-multimodal-retrieval-augmented-generation/#:~:text=When%20a%20user%20prompts%20the,for%20generating%20the%20final%20response)).
- **Knowledge Base Construction** – For documents like PDFs that have text and images, a common approach is to preprocess them into chunks of text and image (figure) embeddings separately ([An Easy Introduction to Multimodal Retrieval-Augmented Generation | NVIDIA Technical Blog](https://developer.nvidia.com/blog/an-easy-introduction-to-multimodal-retrieval-augmented-generation/#:~:text=Image%3A%20Diagram%20shows%20extracting%20text%2C,tackling%20different%20types%20of%20images)) ([An Easy Introduction to Multimodal Retrieval-Augmented Generation | NVIDIA Technical Blog](https://developer.nvidia.com/blog/an-easy-introduction-to-multimodal-retrieval-augmented-generation/#:~:text=)). During a search, both types of chunks can be retrieved. For example, if a query asks “What does the chart show in the sales report?”, the system might retrieve the chart image’s caption or the chart image itself as a chunk, and then use an image-to-text model to interpret it before answering. This highlights an architectural need: sometimes retrieved results need post-processing (e.g., an image result might need OCR or captioning before final use).
- **Late Interaction Models** – Beyond simple vector similarity, advanced architectures use late interaction (like ColBERT for text, or its multimodal variants) where rather than a single vector, multiple vectors per item (e.g. per word or per image region) are stored. This can increase recall and allow more fine-grained matching (e.g. ensuring that for an image of “blue car”, the word “blue” in query specifically matches color features, etc.). Such models often require custom scoring on retrieval (not just dot product). They are more complex but can boost precision in cross-modal search.
- **Scaling and Latency** – Architecturally, a multimodal search system often has an **offline encoding pipeline** (for images, videos, etc., using GPUs to generate embeddings) and an **online serving** component (for queries). To meet latency targets, some deploy lighter/faster encoders for query side (or even use two-tier: a fast coarse search followed by a slower re-rank using a heavier model). Vector indexes like FAISS or ScaNN are used for speed. Moreover, caching frequent multimodal queries and using CDNs for image results can be important in real-world deployments.
- **Feedback and Relevance Learning** – Another pattern is using user feedback signals (clicks, dwell time) to refine multimodal search results. For instance, a system might start with CLIP embeddings but then train a shallow re-ranking model based on what users clicked. This effectively adjusts the embedding space or ranking function to better suit the specific domain (e.g., emphasize certain features). It’s analogous to learning-to-rank in text search but now can involve multimodal features (like “users often click results where the image style is similar”).
- **Emerging: Multimodal LLM as Retriever** – A nascent but interesting pattern: using a multimodal large language model itself to perform retrieval by *directly reading* a set of documents/images. For example, GPT-4 can be given a set of captioned images and asked which ones match a query. This doesn’t scale like vector search, but for small corpora it can work. Another twist is using an LLM to generate text queries from an image (i.e., “describe this image in words” then do text search). These hybrid strategies blur the line between retrieval and reasoning, and might grow as models get more capable.

In essence, designing a multimodal search system involves combining multiple components (encoders, vector stores, possibly RAG with LLMs) in a thoughtful way. The references and diagram provided give a blueprint of how these pieces can fit together in a production system ([An Easy Introduction to Multimodal Retrieval-Augmented Generation | NVIDIA Technical Blog](https://developer.nvidia.com/blog/an-easy-introduction-to-multimodal-retrieval-augmented-generation/#:~:text=When%20a%20user%20prompts%20the,for%20generating%20the%20final%20response)) ([An Easy Introduction to Multimodal Retrieval-Augmented Generation | NVIDIA Technical Blog](https://developer.nvidia.com/blog/an-easy-introduction-to-multimodal-retrieval-augmented-generation/#:~:text=Image%3A%20Diagram%20shows%20a%20workflow,retrieving%20Information%20from%20multimodal%20data)). As toolkit libraries and models evolve, we expect to see even more unified architectures (e.g., a single model that can directly index and retrieve from raw multimodal documents). 

---

*This README is intended as a living document – contributions are welcome! Whether it’s a new dataset, library, or insightful article, please feel free to open a PR to add resources. Multimodal search is a fast-moving field, and keeping this list up-to-date will help everyone build smarter systems that see and hear beyond just text.*
